{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import youtube_dl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'',\n",
       " b\"ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\\n  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\\n  configuration: --prefix=/tmp/build/80754af9/ffmpeg_1587154242452/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho --cc=/tmp/build/80754af9/ffmpeg_1587154242452/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\\n  libavutil      56. 31.100 / 56. 31.100\\n  libavcodec     58. 54.100 / 58. 54.100\\n  libavformat    58. 29.100 / 58. 29.100\\n  libavdevice    58.  8.100 / 58.  8.100\\n  libavfilter     7. 57.100 /  7. 57.100\\n  libavresample   4.  0.  0 /  4.  0.  0\\n  libswscale      5.  5.100 /  5.  5.100\\n  libswresample   3.  5.100 /  3.  5.100\\n  libpostproc    55.  5.100 / 55.  5.100\\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from 'videoplayback.mp4':\\n  Metadata:\\n    major_brand     : dash\\n    minor_version   : 0\\n    compatible_brands: iso6avc1mp41\\n    creation_time   : 2023-02-02T19:46:41.000000Z\\n  Duration: 00:02:22.10, start: 0.000000, bitrate: 974 kb/s\\n    Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 854x480 [SAR 1:1 DAR 427:240], 38 kb/s, 23.98 fps, 23.98 tbr, 24k tbn, 47.95 tbc (default)\\n    Metadata:\\n      creation_time   : 2023-02-02T19:46:41.000000Z\\n      handler_name    : ISO Media file produced by Google Inc.\\nStream mapping:\\n  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\\nPress [q] to stop, [?] for help\\n[swscaler @ 0x55fa6504f480] deprecated pixel format used, make sure you did set range correctly\\nOutput #0, image2, to 'frames/frame-%3d.jpg':\\n  Metadata:\\n    major_brand     : dash\\n    minor_version   : 0\\n    compatible_brands: iso6avc1mp41\\n    encoder         : Lavf58.29.100\\n    Stream #0:0(und): Video: mjpeg, yuvj420p(pc), 854x480 [SAR 1:1 DAR 427:240], q=2-31, 200 kb/s, 23.98 fps, 23.98 tbn, 23.98 tbc (default)\\n    Metadata:\\n      creation_time   : 2023-02-02T19:46:41.000000Z\\n      handler_name    : ISO Media file produced by Google Inc.\\n      encoder         : Lavc58.54.100 mjpeg\\n    Side data:\\n      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1\\nframe=  146 fps=0.0 q=24.8 size=N/A time=00:00:06.08 bitrate=N/A speed=12.1x    \\rframe=  303 fps=301 q=24.8 size=N/A time=00:00:12.63 bitrate=N/A speed=12.6x    \\rframe=  457 fps=303 q=24.8 size=N/A time=00:00:19.06 bitrate=N/A speed=12.6x    \\rframe=  603 fps=300 q=24.8 size=N/A time=00:00:25.15 bitrate=N/A speed=12.5x    \\rframe=  720 fps=301 q=24.8 Lsize=N/A time=00:00:30.03 bitrate=N/A speed=12.6x    \\nvideo:9732kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\\n\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_name = \"videoplayback.mp4\"\n",
    "frames = \"frames/\"\n",
    "ffmpeg.input(video_name, t=30).output('frames/frame-%3d.jpg', start_number=0).overwrite_output().run(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_dir = [frames + f for f in os.listdir(frames) if f.endswith(\".jpg\")]\n",
    "frames_dir.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haar_detector(img,scale_fac = 1.1):\n",
    "    grayscale_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    size = grayscale_image.shape\n",
    "    face_cascade = cv2.CascadeClassifier('face.xml') \n",
    "    detected_faces = face_cascade.detectMultiScale(grayscale_image,scaleFactor=scale_fac,flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    for (x,y,w,h) in detected_faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    return detected_faces, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = []\n",
    "new_frames = []\n",
    "for i in range(len(frames_dir)):\n",
    "    img = cv2.imread(frames_dir[i])\n",
    "    faces, frame = haar_detector(img)\n",
    "    boxes.append(faces)\n",
    "    new_frames.append(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_fac = [1.01,1.1,2.5]\n",
    "t = []\n",
    "for sc_f in scale_fac:\n",
    "    a = time.time()\n",
    "    for i in range(len(frames_dir)):\n",
    "        img = cv2.imread(frames_dir[i])\n",
    "        haar_detector(img,sc_f)\n",
    "    t.append((time.time() - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[288.0384511947632, 36.95407962799072, 14.027289628982544]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average time taken for each frame is:  0.05132511059443156 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"average time taken for each frame is: \",t[1]/720, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "1. One of the key factors which changes the speed of the algorithm is the scale Factor reducing it increases the time significantly. This is because scale Factor decides how much scaling should happen between previous and next cycle. And when it is low more number of samples are formed so it increases the time of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')  # Codec for AVI format\n",
    "out = cv2.VideoWriter('face_detect.mp4', fourcc, 24.0, (854, 480))\n",
    "\n",
    "for frame in new_frames:\n",
    "    out.write(frame)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to video: https://drive.google.com/file/d/1VvHln7TNbIc6UIcEOmSMFcyvAAvgHgzO/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "1. If the eyes are closed then detector does not detect a face. This issue is because haar filter for eyes did not detect closed eyes and the Viola Jones Algorithm then did not have eyes as a feature and so it did not detect a face.\n",
    "2. Ears are being detected as a face. It is because the xml file was for frontal face detection and but the person was standing sideways and so the algorithm incorrect part as face.\n",
    "3. In some frames tree looks like a face and since haar features are binary masks they cannot differentiate between skin colors and hence since it was structurally looking as a face so the algorithm detected it as a face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(bbox1, bbox2):\n",
    "    \n",
    "    x1, y1, width1, height1 = bbox1\n",
    "    x2, y2, width2, height2 = bbox2\n",
    "\n",
    "    intersect_left = max(x1, x2)\n",
    "    intersect_top = max(y1, y2)\n",
    "    intersect_right = min(x1 + width1, x2 + width2)\n",
    "    intersect_bottom = min(y1 + height1, y2 + height2)\n",
    "\n",
    "    if intersect_right < intersect_left or intersect_bottom < intersect_top:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = (intersect_right - intersect_left) * (intersect_bottom - intersect_top)\n",
    "    area1 = width1 * height1\n",
    "    area2 = width2 * height2\n",
    "\n",
    "    union = area1 + area2 - intersection\n",
    "    return intersection / union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_id = 1\n",
    "previous_labels = []\n",
    "processed_frames = []\n",
    "\n",
    "for i in range(1, len(frames_dir)):\n",
    "    current_frame = cv2.imread(frames_dir[i]).copy()\n",
    "    current_labels = []\n",
    "    for j in range(len(boxes[i])):\n",
    "        current_label = None\n",
    "        current_bbox = boxes[i][j]\n",
    "        for prev_index in range(len(previous_labels)):\n",
    "            previous_bbox = boxes[i-1][prev_index]\n",
    "            if calculate_iou(current_bbox, previous_bbox) > 0.5:\n",
    "                current_label = previous_labels[prev_index]\n",
    "                break\n",
    "        if current_label is None:\n",
    "            current_label = object_id\n",
    "            object_id += 1\n",
    "        current_frame = cv2.rectangle(current_frame, (current_bbox[0], current_bbox[1]), (current_bbox[0] + current_bbox[2], current_bbox[1] + current_bbox[3]), (0,0,255), 5)\n",
    "        current_frame = cv2.putText(current_frame, str(current_label), (current_bbox[0], current_bbox[1] + current_bbox[3]), vis_font, vis_font_scale, vis_color, vis_thickness, cv2.LINE_AA)\n",
    "        current_labels.append(current_label)\n",
    "    previous_labels = current_labels\n",
    "    processed_frames.append(current_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')  # Codec for AVI format\n",
    "out = cv2.VideoWriter('face_detect_label.mp4', fourcc, 24.0, (854, 480))\n",
    "\n",
    "for frame in processed_frames:\n",
    "    out.write(frame)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to video: https://drive.google.com/file/d/1coXrJ-e6iMMVTkTpzscIEN2dXTHk7uzr/view?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_assign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
